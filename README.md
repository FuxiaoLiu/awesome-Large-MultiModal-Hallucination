# 😵LVLM-Hallucination
We can split the hallucination in LVLMs into Three directions, including Detect, Evaluation, Mitigate.

## 🧐Detect
|  Name   | Link  | Github|
|  ----  | ----  | ----|
| Detecting and Preventing Hallucinations in Large Vision Language Models  | [arxiv](https://arxiv.org/abs/2308.06394) | not release |
## 🤓Evaluation
|  Name   | Link  | Github|
|  ----  | ----  | ----|
| POPE:  Evaluating Object Hallucination in Large Vision-Language Models   | [arxiv](https://arxiv.org/abs/2305.10355) | ![Static Badge](https://img.shields.io/badge/github-https%3A%2F%2Fgithub.com%2FRUCAIBox%2FPOPE-blue?logo=github) |
| Evaluation and Analysis of Hallucination in Large Vision-Language Models  | [arxiv](https://arxiv.org/abs/2308.15126) | ![Static Badge](https://img.shields.io/badge/github-not_release-blue?logo=github)|
| Evaluating Hallucinations in Chinese Large Language Models| [arxiv](https://arxiv.org/abs/2310.03368) | ![Static Badge](https://img.shields.io/badge/github-https%3A%2F%2Fgithub.com%2Fxiami2019%2FHalluQA-blue?logo=github) |
| HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models  | [arxiv](https://arxiv.org/abs/2310.14566) |<a><img alt="Static Badge" src="https://img.shields.io/badge/github-HallusionBench-blue?logo=github&link=https%3A%2F%2Fgithub.com%2Ftianyi-lab%2FHallusionBench"></a>

|
## 😋Mitigate
|  Name   | Link  | Github|
|  ----  | ----  | ----|
| Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning   | [arxiv](http://arxiv.org/abs/2306.14565) | https://github.com/FuxiaoLiu/LRV-Instruction |
| Analyzing and Mitigating Object Hallucination in Large Vision-Language Models |[arxiv](https://arxiv.org/pdf/2310.00754.pdf)|https://github.com/YiyangZhou/LURE|
| HallE_Switch_Rethinking and Controlling Object Existence Hallucinations in Large Vision-Language Models for Detailed Caption |[arxiv](https://arxiv.org/pdf/2310.01779v1.pdf)| not release |
|Woodpecker: Hallucination Correction for Multimodal Large Language Models|[arxiv](https://arxiv.org/abs/2310.16045)|https://github.com/BradyFU/Woodpecker|

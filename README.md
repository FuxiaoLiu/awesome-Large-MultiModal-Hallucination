# 😵Multi-Modal LLM Hallucination
We can split the hallucination in Multi-Modal LLM into Three directions, including Detect, Evaluation, Mitigate.

## 🧐Detect
|  Name   | Link  | Github|
|  ----  | ----  | ----|
| Detecting and Preventing Hallucinations in Large Vision Language Models  | [![Static Badge](https://img.shields.io/badge/2308.06394-red?logo=arxiv)](https://arxiv.org/abs/2308.06394) | ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github)|

## 🤓Evaluation
|  Name   | Link  | Github|Tags|
|  ----  | ----  | ----|----|
| POPE:  Evaluating Object Hallucination in Large Vision-Language Models   |  [![Static Badge](https://img.shields.io/badge/2305.10355-red?logo=arxiv)](https://arxiv.org/abs/2305.10355) |[![](https://img.shields.io/badge/POPE-black?logo=github)](https://github.com/AoiDragon/POPE)|![Static Badge](https://img.shields.io/badge/Object-green)|
| Evaluation and Analysis of Hallucination in Large Vision-Language Models  |  [![Static Badge](https://img.shields.io/badge/2308.15126-red?logo=arxiv)](https://arxiv.org/abs/2308.15126) | ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github)|
| HalluQA: Evaluating Hallucinations in Chinese Large Language Models| [![Static Badge](https://img.shields.io/badge/2310.03368-red?logo=arxiv)](https://arxiv.org/abs/2310.03368) | ![](https://img.shields.io/badge/not_release-black?logo=github)|![Static Badge](https://img.shields.io/badge/Knowledge-green)|
| HallusionBench: An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models  |  [![Static Badge](https://img.shields.io/badge/2310.14566-red?logo=arxiv)](https://arxiv.org/abs/2310.14566) |[![](https://img.shields.io/badge/HallusionBench-black?logo=github)](https://github.com/tianyi-lab/HallusionBench)|![Static Badge](https://img.shields.io/badge/Knowledge-green)|
## 😋Mitigate
|  Name   | Link  | Github|
|  ----  | ----  | ----|
| Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning   |  [![Static Badge](https://img.shields.io/badge/2306.14565-red?logo=arxiv)](http://arxiv.org/abs/2306.14565) |[![](https://img.shields.io/badge/LRV--Instruction-black?logo=github)]( https://github.com/FuxiaoLiu/LRV-Instruction) |
| Analyzing and Mitigating Object Hallucination in Large Vision-Language Models | [![Static Badge](https://img.shields.io/badge/2310.00754-red?logo=arxiv)](https://arxiv.org/pdf/2310.00754.pdf)|[![](https://img.shields.io/badge/LURE-black?logo=github)](https://github.com/YiyangZhou/LURE)|
| HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision-Language Models for Detailed Caption | [![Static Badge](https://img.shields.io/badge/2310.01779-red?logo=arxiv)](https://arxiv.org/pdf/2310.01779v1.pdf)| ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github) |
| Woodpecker: Hallucination Correction for Multimodal Large Language Models| [![Static Badge](https://img.shields.io/badge/2310.16045-red?logo=arxiv)](https://arxiv.org/abs/2310.16045)|[![](https://img.shields.io/badge/Woodpecker-black?logo=github)](https://github.com/BradyFU/Woodpecker)|

# LVLM-Hallucination
We can split the hallucination in LVLMs into Three directions, including Detect, Evaluation, Mitigate.

## Detect
|  Name   | Link  | Github|
|  ----  | ----  | ----|
| Detecting and Preventing Hallucinations in Large Vision Language Models  | [arxiv](https://arxiv.org/abs/2308.06394) | not release |
## Evaluation
|  Name   | Link  | Github|
|  ----  | ----  | ----|
| POPE:  Evaluating Object Hallucination in Large Vision-Language Models   | [arxiv](https://arxiv.org/abs/2305.10355) | https://github.com/RUCAIBox/POPE |
| Evaluation and Analysis of Hallucination in Large Vision-Language Models  | [arxiv](https://arxiv.org/abs/2308.15126) | not release |
| Evaluating Hallucinations in Chinese Large Language Models| [arxiv](https://arxiv.org/abs/2310.03368) | https://github.com/xiami2019/HalluQA |
| HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models  | [arxiv](https://arxiv.org/abs/2310.14566) | https://github.com/tianyi-lab/HallusionBench |
## Mitigate
|  Name   | Link  | Github|
|  ----  | ----  | ----|
| Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning   | [arxiv](http://arxiv.org/abs/2306.14565) | https://github.com/FuxiaoLiu/LRV-Instruction |
| Analyzing and Mitigating Object Hallucination in Large Vision-Language Models |[arxiv](https://arxiv.org/pdf/2310.00754.pdf)|https://github.com/YiyangZhou/LURE|
| HallE_Switch_Rethinking and Controlling Object Existence Hallucinations in Large Vision-Language Models for Detailed Caption |[arxiv](https://arxiv.org/pdf/2310.01779v1.pdf)| not release |
|Woodpecker: Hallucination Correction for Multimodal Large Language Models|[arxiv](https://arxiv.org/abs/2310.16045)|https://github.com/BradyFU/Woodpecker|

# 😵Multi-Modal LLM Hallucination
We can split the hallucination in Multi-Modal LLM into Three directions, including Detect, Evaluation, Mitigate.

## 🧐Detect
|  Name   | Link  | Github|
|  ----  | ----  | ----|
| Detecting and Preventing Hallucinations in Large Vision Language Models, (Gunjal et al. 2023)  | [![Static Badge](https://img.shields.io/badge/2308.06394-red?logo=arxiv)](https://arxiv.org/abs/2308.06394) | ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github)|
| **HaELM**: Evaluation and Analysis of Hallucination in Large Vision-Language Models, (Wang et al. 2023a)  |  [![Static Badge](https://img.shields.io/badge/2308.15126-red?logo=arxiv)](https://arxiv.org/abs/2308.15126) | ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github)
## 🤓Evaluation
|  Name   | Link  | Github|Tags|
|  ----  | ----  | ----|----|
| **POPE**:  Evaluating Object Hallucination in Large Vision-Language Models   |  [![Static Badge](https://img.shields.io/badge/2305.10355-red?logo=arxiv)](https://arxiv.org/abs/2305.10355) |[![](https://img.shields.io/badge/POPE-black?logo=github)](https://github.com/AoiDragon/POPE)|![Static Badge](https://img.shields.io/badge/Object_Existence-green)|
| **HaELM**: Evaluation and Analysis of Hallucination in Large Vision-Language Models, (Wang et al. 2023a)  |  [![Static Badge](https://img.shields.io/badge/2308.15126-red?logo=arxiv)](https://arxiv.org/abs/2308.15126) | ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github)|![Static Badge](https://img.shields.io/badge/Object-green)|
| **HallusionBench**: An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models  |  [![Static Badge](https://img.shields.io/badge/2310.14566-red?logo=arxiv)](https://arxiv.org/abs/2310.14566) |[![](https://img.shields.io/badge/HallusionBench-black?logo=github)](https://github.com/tianyi-lab/HallusionBench)|![Static Badge](https://img.shields.io/badge/Knowledge-green)|
| **HallE-Switch**: Rethinking and Controlling Object Existence Hallucinations in Large Vision-Language Models for Detailed Caption, (Zhai et al. 2023) | [![Static Badge](https://img.shields.io/badge/2310.01779-red?logo=arxiv)](https://arxiv.org/pdf/2310.01779v1.pdf)| ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github) |![Static Badge](https://img.shields.io/badge/Object-green)![Static Badge](https://img.shields.io/badge/LLM--Evaluation-green)|
| **Bingo**: Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges | [![Static Badge](https://img.shields.io/badge/2311.03287-red?logo=arxiv)](https://arxiv.org/pdf/2311.03287.pdf)| [![](https://img.shields.io/badge/Bingo-black?logo=github)](https://github.com/gzcch/Bingo) |![Static Badge](https://img.shields.io/badge/Object-green)![Static Badge](https://img.shields.io/badge/Knowledge-green)|
| **FaithScore**: Evaluating Hallucinations in Large Vision-Language Models | [![Static Badge](https://img.shields.io/badge/2311.01477-red?logo=arxiv)](https://arxiv.org/pdf/2311.01477.pdf)| [![](https://img.shields.io/badge/FaithScore-black?logo=github)](https://github.com/bcdnlp/faithscore) |![Static Badge](https://img.shields.io/badge/Object-green)|
| **AMBER**: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation| [![Static Badge](https://img.shields.io/badge/2311.07397-red?logo=arxiv)](https://arxiv.org/pdf/2311.07397.pdf)| [![](https://img.shields.io/badge/AMBER-black?logo=github)](https://github.com/junyangwang0410/amber) |![Static Badge](https://img.shields.io/badge/LLM--Free-green)![Static Badge](https://img.shields.io/badge/Object--Existence--Attribute--Relation-green)|
## 😋Mitigate
|  Name   | Link  | Github|Tags|
|  ----  | ----  | ----|----|
| **LRV-Instruction**: Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning   |  [![Static Badge](https://img.shields.io/badge/2306.14565-red?logo=arxiv)](http://arxiv.org/abs/2306.14565) |[![](https://img.shields.io/badge/LRV--Instruction-black?logo=github)]( https://github.com/FuxiaoLiu/LRV-Instruction) ||
| **LURE**: Analyzing and Mitigating Object Hallucination in Large Vision-Language Models, (Zhou et al. 2023b) | [![Static Badge](https://img.shields.io/badge/2310.00754-red?logo=arxiv)](https://arxiv.org/pdf/2310.00754.pdf)|[![](https://img.shields.io/badge/LURE-black?logo=github)](https://github.com/YiyangZhou/LURE)|![Static Badge](https://img.shields.io/badge/LLM--Evaluation-green)|
| **HallE-Switch**: Rethinking and Controlling Object Existence Hallucinations in Large Vision-Language Models for Detailed Caption | [![Static Badge](https://img.shields.io/badge/2310.01779-red?logo=arxiv)](https://arxiv.org/pdf/2310.01779v1.pdf)| ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github) ||
| **Woodpecker**: Hallucination Correction for Multimodal Large Language Models| [![Static Badge](https://img.shields.io/badge/2310.16045-red?logo=arxiv)](https://arxiv.org/abs/2310.16045)|[![](https://img.shields.io/badge/Woodpecker-black?logo=github)](https://github.com/BradyFU/Woodpecker)||
| **LLaVA-RLHF**: Aligning Large Multimodal Models with Factually Augmented RLHF| [![Static Badge](https://img.shields.io/badge/2309.14525-red?logo=arxiv)](https://arxiv.org/abs/2309.14525)|[![](https://img.shields.io/badge/LLaVA--RLHF-black?logo=github)](https://github.com/llava-rlhf/LLaVA-RLHF)||
| **Volcano**: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision| [![Static Badge](https://img.shields.io/badge/2311.07362-red?logo=arxiv)](https://arxiv.org/abs/2311.07362)|[![](https://img.shields.io/badge/Volcano-black?logo=github)](https://github.com/kaistAI/Volcano)||

# Large MultiModal Model Hallucinationüòµ

LMM hallucination refers to occasional instances where MLLMs generate content that appears plausible but deviates from or conflicts with the provided image.

In the MLLM community, we've developed methods for detecting, evaluating, and mitigating hallucinationsüëç.

---

## Detecting

1. **HaELM** : Evaluation and Analysis of Hallucination in Large Vision-Language Models, (Wang et al. 2023a)
   - [![Static Badge](https://img.shields.io/badge/2308.15126-red?logo=arxiv)](https://arxiv.org/abs/2308.15126)   [![](https://img.shields.io/badge/HaELM-black?logo=github)](https://github.com/junyangwang0410/HaELM)
   - An automatic MLLM hallucination detection framework, Train LLM to detect
2. **HallE-Switch** : Rethinking and Controlling Object Existence Hallucinations in Large Vision-Language Models for Detailed Caption, (Zhai et al. 2023)]
   - [![Static Badge](https://img.shields.io/badge/2310.01779-red?logo=arxiv)](https://arxiv.org/pdf/2310.01779v1.pdf) ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github)

## Evaluating

1. **POPE**: Evaluating Object Hallucination in Large Vision-Language Models, (Li et al. EMNLP 2023)
      - [![Static Badge](https://img.shields.io/badge/2305.10355-red?logo=arxiv)](https://arxiv.org/abs/2305.10355) [![](https://img.shields.io/badge/POPE-black?logo=github)](https://github.com/AoiDragon/POPE)
   - Discriminative Task: Object Existence
2. **HaELM** : Evaluation and Analysis of Hallucination in Large Vision-Language Models, (Wang et al. 2023a)
      - [![Static Badge](https://img.shields.io/badge/2308.15126-red?logo=arxiv)](https://arxiv.org/abs/2308.15126)   [![](https://img.shields.io/badge/HaELM-black?logo=github)](https://github.com/junyangwang0410/HaELM)
   - Generative Task
3. **HallusionBench** : An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Model
      - [![Static Badge](https://img.shields.io/badge/2310.14566-red?logo=arxiv)](https://arxiv.org/abs/2310.14566)  [![](https://img.shields.io/badge/HallusionBench-black?logo=github)](https://github.com/tianyi-lab/HallusionBench)
   - Generative Task
4. **HallE-Switch** : Rethinking and Controlling Object Existence Hallucinations in Large Vision-Language Models for Detailed Caption, (Zhai et al. 2023)]
      - [![Static Badge](https://img.shields.io/badge/2310.01779-red?logo=arxiv)](https://arxiv.org/pdf/2310.01779) ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github)
   - Discriminative Task: Object Existence
5. **Bingo** : Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges
      - [![Static Badge](https://img.shields.io/badge/2311.03287-red?logo=arxiv)](https://arxiv.org/pdf/2311.03287) [![](https://img.shields.io/badge/Bingo-black?logo=github)](https://github.com/gzcch/Bingo)
6. **FaithScore** : Evaluating Hallucinations in Large Vision-Language Models
      - [![Static Badge](https://img.shields.io/badge/2311.01477-red?logo=arxiv)](https://arxiv.org/pdf/2311.01477)  [![](https://img.shields.io/badge/FaithScore-black?logo=github)](https://github.com/bcdnlp/faithscore)
7. **AMBER** : An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation
      - [![Static Badge](https://img.shields.io/badge/2311.07397-red?logo=arxiv)](https://arxiv.org/pdf/2311.07397)  [![](https://img.shields.io/badge/AMBER-black?logo=github)](https://github.com/junyangwang0410/amber)

---

## Mitigating

1. **LRV-Instruction** : Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning
   - [![Static Badge](https://img.shields.io/badge/2306.14565-red?logo=arxiv)](http://arxiv.org/abs/2306.14565)  [![](https://img.shields.io/badge/LRV--Instruction-black?logo=github)](https://github.com/FuxiaoLiu/LRV-Instruction)
2. **LURE** : Analyzing and Mitigating Object Hallucination in Large Vision-Language Models, (Zhou et al. 2023b)
   - [![Static Badge](https://img.shields.io/badge/2310.00754-red?logo=arxiv)](https://arxiv.org/pdf/2310.00754) [![](https://img.shields.io/badge/LURE-black?logo=github)](https://github.com/YiyangZhou/LURE)
3. **HallE-Switch** : Rethinking and Controlling Object Existence Hallucinations in Large Vision-Language Models for Detailed Caption
   - [![Static Badge](https://img.shields.io/badge/2310.01779-red?logo=arxiv)](https://arxiv.org/pdf/2310.01779)  ![Static Badge](https://img.shields.io/badge/not_release-black?logo=github)
4. **Woodpecker** : Hallucination Correction for Multimodal Large Language Models
   - [![Static Badge](https://img.shields.io/badge/2310.16045-red?logo=arxiv)](https://arxiv.org/abs/2310.16045) [![](https://img.shields.io/badge/Woodpecker-black?logo=github)](https://github.com/BradyFU/Woodpecker)
5. **LLaVA-RLHF** : Aligning Large Multimodal Models with Factually Augmented RLHF
   - [![Static Badge](https://img.shields.io/badge/2309.14525-red?logo=arxiv)](https://arxiv.org/abs/2309.14525) [![](https://img.shields.io/badge/LLaVA--RLHF-black?logo=github)](https://github.com/llava-rlhf/LLaVA-RLHF)
6. **Volcano** : Mitigating Multimodal Hallucination through Self-Feedback Guided Revision
   - [![Static Badge](https://img.shields.io/badge/2311.07362-red?logo=arxiv)](https://arxiv.org/abs/2311.07362) [![](https://img.shields.io/badge/Volcano-black?logo=github)](https://github.com/kaistAI/Volcano)
